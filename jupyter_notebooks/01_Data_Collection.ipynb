{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aStgWSO0E0E"
      },
      "source": [
        "# **Data Collection - Brain Tumor Detector**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eLEkw5O0ECa"
      },
      "source": [
        "## Objectives\n",
        "\n",
        "* Import the necessary packages\n",
        "* Set working directory\n",
        "* Fetch image dataset for Kaggle but import manually due to Gitpod size restrictions of 100MB\n",
        "* Clean data so that only image files are used\n",
        "* Split the data into Test, Train and Validation folders\n",
        "\n",
        "## Inputs\n",
        "\n",
        "* Kaggle Dataset: https://www.kaggle.com/datasets/preetviradiya/brian-tumor-dataset\n",
        "* Kaggle JSON file - authentication token\n",
        "\n",
        "## Outputs\n",
        "\n",
        "* Once the dataset is imported, it will have the following structure:\n",
        "\n",
        "* inputs\n",
        "    * mri-scans\n",
        "        *test\n",
        "            * healthy\n",
        "            * tumor\n",
        "        *train\n",
        "            * healthy\n",
        "            * tumor\n",
        "        * validation\n",
        "            * healthy\n",
        "            * tumor\n",
        "\n",
        "\n",
        "## Comments/Conclusions\n",
        "\n",
        "* These steps are all necessary to fetch the data, clean it and then divide it up into subsets for the purposes of machine learning. In the next step, we will look into Data Visualisation to understand the data and discover patterns that will emerge. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9uWZXH9LwoQg"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.19.2\n",
            "  Downloading numpy-1.19.2-cp38-cp38-manylinux2010_x86_64.whl (14.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.5/14.5 MB\u001b[0m \u001b[31m77.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting pandas==1.1.2\n",
            "  Downloading pandas-1.1.2-cp38-cp38-manylinux1_x86_64.whl (10.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mmm\n",
            "\u001b[?25hCollecting matplotlib==3.3.1\n",
            "  Downloading matplotlib-3.3.1-cp38-cp38-manylinux1_x86_64.whl (11.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
            "\u001b[?25hCollecting seaborn==0.11.0\n",
            "  Downloading seaborn-0.11.0-py3-none-any.whl (283 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m283.1/283.1 kB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting plotly==4.12.0\n",
            "  Downloading plotly-4.12.0-py2.py3-none-any.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: streamlit==0.85.0 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from -r /workspace/brain-tumor-detector/requirements.txt (line 8)) (0.85.0)\n",
            "Collecting scikit-learn==0.24.2\n",
            "  Downloading scikit_learn-0.24.2-cp38-cp38-manylinux2010_x86_64.whl (24.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.9/24.9 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-cpu==2.6.0\n",
            "  Downloading tensorflow_cpu-2.6.0-cp38-cp38-manylinux2010_x86_64.whl (172.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m172.4/172.4 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting keras==2.6.0\n",
            "  Downloading keras-2.6.0-py2.py3-none-any.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m102.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting protobuf==3.20\n",
            "  Downloading protobuf-3.20.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m117.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<5 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from -r /workspace/brain-tumor-detector/requirements.txt (line 14)) (4.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from pandas==1.1.2->-r /workspace/brain-tumor-detector/requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.2 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from pandas==1.1.2->-r /workspace/brain-tumor-detector/requirements.txt (line 3)) (2024.1)\n",
            "Collecting kiwisolver>=1.0.1\n",
            "  Downloading kiwisolver-1.4.5-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m111.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi>=2020.06.20 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from matplotlib==3.3.1->-r /workspace/brain-tumor-detector/requirements.txt (line 4)) (2024.2.2)\n",
            "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3\n",
            "  Downloading pyparsing-3.1.1-py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.1/103.1 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow>=6.2.0 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from matplotlib==3.3.1->-r /workspace/brain-tumor-detector/requirements.txt (line 4)) (10.2.0)\n",
            "Collecting cycler>=0.10\n",
            "  Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
            "Collecting scipy>=1.0\n",
            "  Downloading scipy-1.10.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from plotly==4.12.0->-r /workspace/brain-tumor-detector/requirements.txt (line 6)) (1.16.0)\n",
            "Collecting retrying>=1.3.3\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: toml in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from streamlit==0.85.0->-r /workspace/brain-tumor-detector/requirements.txt (line 8)) (0.10.2)\n",
            "Requirement already satisfied: astor in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from streamlit==0.85.0->-r /workspace/brain-tumor-detector/requirements.txt (line 8)) (0.8.1)\n",
            "Requirement already satisfied: base58 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from streamlit==0.85.0->-r /workspace/brain-tumor-detector/requirements.txt (line 8)) (2.1.1)\n",
            "Requirement already satisfied: cachetools>=4.0 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from streamlit==0.85.0->-r /workspace/brain-tumor-detector/requirements.txt (line 8)) (5.3.2)\n",
            "Requirement already satisfied: blinker in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from streamlit==0.85.0->-r /workspace/brain-tumor-detector/requirements.txt (line 8)) (1.7.0)\n",
            "Requirement already satisfied: gitpython in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from streamlit==0.85.0->-r /workspace/brain-tumor-detector/requirements.txt (line 8)) (3.1.42)\n",
            "Requirement already satisfied: validators in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from streamlit==0.85.0->-r /workspace/brain-tumor-detector/requirements.txt (line 8)) (0.22.0)\n",
            "Requirement already satisfied: requests in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from streamlit==0.85.0->-r /workspace/brain-tumor-detector/requirements.txt (line 8)) (2.31.0)\n",
            "Requirement already satisfied: click<8.0,>=7.0 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from streamlit==0.85.0->-r /workspace/brain-tumor-detector/requirements.txt (line 8)) (7.1.2)\n",
            "Requirement already satisfied: tzlocal in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from streamlit==0.85.0->-r /workspace/brain-tumor-detector/requirements.txt (line 8)) (5.2)\n",
            "Requirement already satisfied: pydeck>=0.1.dev5 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from streamlit==0.85.0->-r /workspace/brain-tumor-detector/requirements.txt (line 8)) (0.8.1b0)\n",
            "Requirement already satisfied: attrs in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from streamlit==0.85.0->-r /workspace/brain-tumor-detector/requirements.txt (line 8)) (23.2.0)\n",
            "Requirement already satisfied: pyarrow in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from streamlit==0.85.0->-r /workspace/brain-tumor-detector/requirements.txt (line 8)) (15.0.0)\n",
            "Requirement already satisfied: tornado>=5.0 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from streamlit==0.85.0->-r /workspace/brain-tumor-detector/requirements.txt (line 8)) (6.4)\n",
            "Requirement already satisfied: watchdog in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from streamlit==0.85.0->-r /workspace/brain-tumor-detector/requirements.txt (line 8)) (4.0.0)\n",
            "Requirement already satisfied: packaging in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from streamlit==0.85.0->-r /workspace/brain-tumor-detector/requirements.txt (line 8)) (23.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from scikit-learn==0.24.2->-r /workspace/brain-tumor-detector/requirements.txt (line 10)) (1.3.2)\n",
            "Collecting threadpoolctl>=2.0.0\n",
            "  Downloading threadpoolctl-3.3.0-py3-none-any.whl (17 kB)\n",
            "Collecting six\n",
            "  Downloading six-1.15.0-py2.py3-none-any.whl (10 kB)\n",
            "Collecting flatbuffers~=1.12.0\n",
            "  Downloading flatbuffers-1.12-py2.py3-none-any.whl (15 kB)\n",
            "Collecting h5py~=3.1.0\n",
            "  Downloading h5py-3.1.0-cp38-cp38-manylinux1_x86_64.whl (4.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m74.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting termcolor~=1.1.0\n",
            "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting astunparse~=1.6.3\n",
            "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
            "Collecting gast==0.4.0\n",
            "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
            "Collecting opt-einsum~=3.3.0\n",
            "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wrapt~=1.12.1\n",
            "  Downloading wrapt-1.12.1.tar.gz (27 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting keras-preprocessing~=1.1.2\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard~=2.6\n",
            "  Downloading tensorboard-2.14.0-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m42.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n",
            "\u001b[?25hCollecting grpcio<2.0,>=1.37.0\n",
            "  Downloading grpcio-1.60.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.4/5.4 MB\u001b[0m \u001b[31m101.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting google-pasta~=0.2\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting wheel~=0.35\n",
            "  Downloading wheel-0.42.0-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.4/65.4 kB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator~=2.6\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m74.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions~=3.7.4\n",
            "  Downloading typing_extensions-3.7.4.3-py3-none-any.whl (22 kB)\n",
            "Collecting clang~=5.0\n",
            "  Downloading clang-5.0.tar.gz (30 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hCollecting absl-py~=0.10\n",
            "  Downloading absl_py-0.15.0-py3-none-any.whl (132 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.0/132.0 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from altair<5->-r /workspace/brain-tumor-detector/requirements.txt (line 14)) (0.4)\n",
            "Requirement already satisfied: jinja2 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from altair<5->-r /workspace/brain-tumor-detector/requirements.txt (line 14)) (3.1.3)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from altair<5->-r /workspace/brain-tumor-detector/requirements.txt (line 14)) (4.21.1)\n",
            "Requirement already satisfied: toolz in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from altair<5->-r /workspace/brain-tumor-detector/requirements.txt (line 14)) (0.12.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from jsonschema>=3.0->altair<5->-r /workspace/brain-tumor-detector/requirements.txt (line 14)) (0.18.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from jsonschema>=3.0->altair<5->-r /workspace/brain-tumor-detector/requirements.txt (line 14)) (2023.12.1)\n",
            "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from jsonschema>=3.0->altair<5->-r /workspace/brain-tumor-detector/requirements.txt (line 14)) (1.3.10)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from jsonschema>=3.0->altair<5->-r /workspace/brain-tumor-detector/requirements.txt (line 14)) (6.1.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from jsonschema>=3.0->altair<5->-r /workspace/brain-tumor-detector/requirements.txt (line 14)) (0.33.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from jinja2->altair<5->-r /workspace/brain-tumor-detector/requirements.txt (line 14)) (2.1.5)\n",
            "Collecting scipy>=1.0\n",
            "  Downloading scipy-1.10.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.5/34.5 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25h  Downloading scipy-1.9.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (33.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.8/33.8 MB\u001b[0m \u001b[31m30.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
            "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
            "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
            "Collecting markdown>=2.6.8\n",
            "  Downloading Markdown-3.5.2-py3-none-any.whl (103 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m103.9/103.9 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorboard-data-server<0.8.0,>=0.7.0\n",
            "  Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m100.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
            "\u001b[?25hCollecting werkzeug>=1.0.1\n",
            "  Downloading werkzeug-3.0.1-py3-none-any.whl (226 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.7/226.7 kB\u001b[0m \u001b[31m80.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /home/gitpod/.pyenv/versions/3.8.18/lib/python3.8/site-packages (from tensorboard~=2.6->tensorflow-cpu==2.6.0->-r /workspace/brain-tumor-detector/requirements.txt (line 11)) (56.0.0)\n",
            "Collecting google-auth<3,>=1.6.3\n",
            "  Downloading google_auth-2.28.0-py2.py3-none-any.whl (186 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m186.9/186.9 kB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from requests->streamlit==0.85.0->-r /workspace/brain-tumor-detector/requirements.txt (line 8)) (3.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from requests->streamlit==0.85.0->-r /workspace/brain-tumor-detector/requirements.txt (line 8)) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from requests->streamlit==0.85.0->-r /workspace/brain-tumor-detector/requirements.txt (line 8)) (2.2.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from gitpython->streamlit==0.85.0->-r /workspace/brain-tumor-detector/requirements.txt (line 8)) (4.0.11)\n",
            "Requirement already satisfied: backports.zoneinfo in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from tzlocal->streamlit==0.85.0->-r /workspace/brain-tumor-detector/requirements.txt (line 8)) (0.2.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->gitpython->streamlit==0.85.0->-r /workspace/brain-tumor-detector/requirements.txt (line 8)) (5.0.1)\n",
            "Collecting rsa<5,>=3.1.4\n",
            "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
            "Collecting pyasn1-modules>=0.2.1\n",
            "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m71.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-oauthlib>=0.7.0\n",
            "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema>=3.0->altair<5->-r /workspace/brain-tumor-detector/requirements.txt (line 14)) (3.17.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow-cpu==2.6.0->-r /workspace/brain-tumor-detector/requirements.txt (line 11)) (7.0.1)\n",
            "Collecting pyasn1<0.6.0,>=0.4.6\n",
            "  Downloading pyasn1-0.5.1-py2.py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.9/84.9 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting oauthlib>=3.0.0\n",
            "  Downloading oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m151.7/151.7 kB\u001b[0m \u001b[31m42.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: wrapt, typing-extensions, termcolor, keras, flatbuffers, clang, wheel, werkzeug, threadpoolctl, tensorflow-estimator, tensorboard-data-server, six, pyparsing, pyasn1, protobuf, oauthlib, numpy, kiwisolver, grpcio, gast, cycler, scipy, rsa, retrying, requests-oauthlib, pyasn1-modules, opt-einsum, markdown, keras-preprocessing, h5py, google-pasta, astunparse, absl-py, scikit-learn, plotly, pandas, matplotlib, google-auth, seaborn, google-auth-oauthlib, tensorboard, tensorflow-cpu\n",
            "\u001b[33m  DEPRECATION: wrapt is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
            "\u001b[0m  Running setup.py install for wrapt ... \u001b[?25ldone\n",
            "\u001b[?25h  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.9.0\n",
            "    Uninstalling typing_extensions-4.9.0:\n",
            "      Successfully uninstalled typing_extensions-4.9.0\n",
            "\u001b[33m  DEPRECATION: termcolor is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
            "\u001b[0m  Running setup.py install for termcolor ... \u001b[?25ldone\n",
            "\u001b[?25h\u001b[33m  DEPRECATION: clang is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
            "\u001b[0m  Running setup.py install for clang ... \u001b[?25ldone\n",
            "\u001b[?25h  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 4.25.2\n",
            "    Uninstalling protobuf-4.25.2:\n",
            "      Successfully uninstalled protobuf-4.25.2\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.24.4\n",
            "    Uninstalling numpy-1.24.4:\n",
            "      Successfully uninstalled numpy-1.24.4\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.0.3\n",
            "    Uninstalling pandas-2.0.3:\n",
            "      Successfully uninstalled pandas-2.0.3\n",
            "Successfully installed absl-py-0.15.0 astunparse-1.6.3 clang-5.0 cycler-0.12.1 flatbuffers-1.12 gast-0.4.0 google-auth-2.28.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.60.1 h5py-3.1.0 keras-2.6.0 keras-preprocessing-1.1.2 kiwisolver-1.4.5 markdown-3.5.2 matplotlib-3.3.1 numpy-1.19.2 oauthlib-3.2.2 opt-einsum-3.3.0 pandas-1.1.2 plotly-4.12.0 protobuf-3.20.0 pyasn1-0.5.1 pyasn1-modules-0.3.0 pyparsing-3.1.1 requests-oauthlib-1.3.1 retrying-1.3.4 rsa-4.9 scikit-learn-0.24.2 scipy-1.9.3 seaborn-0.11.0 six-1.15.0 tensorboard-2.14.0 tensorboard-data-server-0.7.2 tensorflow-cpu-2.6.0 tensorflow-estimator-2.15.0 termcolor-1.1.0 threadpoolctl-3.3.0 typing-extensions-3.7.4.3 werkzeug-3.0.1 wheel-0.42.0 wrapt-1.12.1\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -r /workspace/brain-tumor-detector/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqP-UeN-z3i2"
      },
      "source": [
        "## Change working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The notebooks are in a sub-folder of this directory, therefore, when running the notebook in this editor it's a necessary requirement to change the working directory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aOGIGS-uz3i2"
      },
      "source": [
        "We need to change the working directory from its current folder to its parent folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "wZfF_j-Bz3i4",
        "outputId": "66943449-1436-4c3d-85c7-b85f9f78349b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/workspace/brain-tumor-detector'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MWW8E7lz3i7"
      },
      "source": [
        "We want to make the parent of the current directory the new current directory\n",
        "* os.path.dirname() gets the parent directory\n",
        "* os.chir() defines the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "TwHsQRWjz3i9",
        "outputId": "86849db3-cd2f-4cc5-ebb8-2d0caafa1a2c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You set a new current directory\n"
          ]
        }
      ],
      "source": [
        "os.chdir(os.path.dirname(current_dir))\n",
        "print(\"You set a new current directory\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_xPk_Ijz3i-"
      },
      "source": [
        "Confirm the new current directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "vz3S-_kjz3jA",
        "outputId": "00b79ae4-75d0-4a96-d193-ac9ef9847ea2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'/workspace'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "current_dir = os.getcwd()\n",
        "current_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-mavJ8DibrcQ"
      },
      "source": [
        "## Install Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting kaggle==1.5.12\n",
            "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25hRequirement already satisfied: six>=1.10 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from kaggle==1.5.12) (1.16.0)\n",
            "Requirement already satisfied: certifi in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from kaggle==1.5.12) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from kaggle==1.5.12) (2.8.2)\n",
            "Requirement already satisfied: requests in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from kaggle==1.5.12) (2.31.0)\n",
            "Collecting tqdm\n",
            "  Downloading tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.3/78.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-slugify\n",
            "  Downloading python_slugify-8.0.4-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: urllib3 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from kaggle==1.5.12) (2.2.0)\n",
            "Collecting text-unidecode>=1.3\n",
            "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 kB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from requests->kaggle==1.5.12) (3.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /workspace/.pyenv_mirror/user/3.8.18/lib/python3.8/site-packages (from requests->kaggle==1.5.12) (3.3.2)\n",
            "Installing collected packages: text-unidecode, tqdm, python-slugify, kaggle\n",
            "\u001b[33m  DEPRECATION: kaggle is being installed using the legacy 'setup.py install' method, because it does not have a 'pyproject.toml' and the 'wheel' package is not installed. pip 23.1 will enforce this behaviour change. A possible replacement is to enable the '--use-pep517' option. Discussion can be found at https://github.com/pypa/pip/issues/8559\u001b[0m\u001b[33m\n",
            "\u001b[0m  Running setup.py install for kaggle ... \u001b[?25ldone\n",
            "\u001b[?25hSuccessfully installed kaggle-1.5.12 python-slugify-8.0.4 text-unidecode-1.3 tqdm-4.66.2\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install kaggle==1.5.12"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Set up the Kaggle JSON file and directory setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.environ['KAGGLE_CONFIG_DIR'] = os.getcwd()\n",
        "! chmod 600 kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Dataset has to be manually imported as the original dataset is bigger than 100MB, therefore giving Gitpod issues with size limits.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY3l0-AxO93d"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFQo3ycuO-v6"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data Cleaning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check and remove non-image files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_non_image_file(my_data_dir):\n",
        "    image_extension = ('.png', '.jpg', '.jpeg')\n",
        "    folders = os.listdir(my_data_dir)\n",
        "    for folder in folders:\n",
        "        files = os.listdir(my_data_dir + '/' + folder)\n",
        "        # print(files)\n",
        "        i = []\n",
        "        j = []\n",
        "        for given_file in files:\n",
        "            if not given_file.lower().endswith(image_extension):\n",
        "                file_location = my_data_dir + '/' + folder + '/' + given_file\n",
        "                os.remove(file_location)  # remove non image files\n",
        "                i.append(1)\n",
        "            else:\n",
        "                j.append(1)\n",
        "                pass\n",
        "        print(f\"Folder: {folder} - has image file\", len(j))\n",
        "        print(f\"Folder: {folder} - has non-image file\", len(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Folder: healthy - has image file 1922\n",
            "Folder: healthy - has non-image file 0\n",
            "Folder: tumor - has image file 1788\n",
            "Folder: tumor - has non-image file 0\n"
          ]
        }
      ],
      "source": [
        "remove_non_image_file(my_data_dir='input/mri-scans')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split train validation test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import joblib\n",
        "\n",
        "\n",
        "def split_train_validation_test_images(my_data_dir, train_set_ratio, validation_set_ratio, test_set_ratio):\n",
        "\n",
        "    if train_set_ratio + validation_set_ratio + test_set_ratio != 1.0:\n",
        "        print(\"train_set_ratio + validation_set_ratio + test_set_ratio should sum to 1.0\")\n",
        "        return\n",
        "\n",
        "    # gets classes labels\n",
        "    labels = os.listdir(my_data_dir)  # it should get only the folder name\n",
        "    if 'test' in labels:\n",
        "        pass\n",
        "    else:\n",
        "        # create train, test folders with classes labels sub-folder\n",
        "        for folder in ['train', 'validation', 'test']:\n",
        "            for label in labels:\n",
        "                os.makedirs(name=my_data_dir + '/' + folder + '/' + label)\n",
        "\n",
        "        for label in labels:\n",
        "\n",
        "            files = os.listdir(my_data_dir + '/' + label)\n",
        "            random.shuffle(files)\n",
        "\n",
        "            train_set_files_qty = int(len(files) * train_set_ratio)\n",
        "            validation_set_files_qty = int(len(files) * validation_set_ratio)\n",
        "\n",
        "            count = 1\n",
        "            for file_name in files:\n",
        "                if count <= train_set_files_qty:\n",
        "                    # move a given file to the train set\n",
        "                    shutil.move(my_data_dir + '/' + label + '/' + file_name,\n",
        "                                my_data_dir + '/train/' + label + '/' + file_name)\n",
        "\n",
        "                elif count <= (train_set_files_qty + validation_set_files_qty):\n",
        "                    # move a given file to the validation set\n",
        "                    shutil.move(my_data_dir + '/' + label + '/' + file_name,\n",
        "                                my_data_dir + '/validation/' + label + '/' + file_name)\n",
        "\n",
        "                else:\n",
        "                    # move given file to test set\n",
        "                    shutil.move(my_data_dir + '/' + label + '/' + file_name,\n",
        "                                my_data_dir + '/test/' + label + '/' + file_name)\n",
        "\n",
        "                count += 1\n",
        "\n",
        "            os.rmdir(my_data_dir + '/' + label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* The training set is divided into a 0.70 ratio of data.\n",
        "* The validation set is divided into a 0.10 ratio of data.\n",
        "* The test set is divided into a 0.20 ratio of data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "split_train_validation_test_images(my_data_dir=f\"input/mri-scans\",\n",
        "                                   train_set_ratio=0.7,\n",
        "                                   validation_set_ratio=0.1,\n",
        "                                   test_set_ratio=0.2\n",
        "                                   )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ltNetd085qHf"
      },
      "source": [
        "## Push files to Repo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* If necessary, uncomment the code below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKlnIozA4eQO",
        "outputId": "fd09bc1f-adb1-4511-f6ce-492a6af570c0"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# try:\n",
        "#   # create here your folder\n",
        "#   # os.makedirs(name='')\n",
        "# except Exception as e:\n",
        "#   print(e)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Data Practitioner Jupyter Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "interpreter": {
      "hash": "8b8334dab9339717f727a1deaf837b322d7a41c20d15cc86be99a8e69ceec8ce"
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 64-bit ('3.8.12': pyenv)",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "orig_nbformat": 2
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
